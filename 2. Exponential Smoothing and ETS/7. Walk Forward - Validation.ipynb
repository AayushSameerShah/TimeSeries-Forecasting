{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f820783-04b4-49c9-bbc6-ab937456b613",
   "metadata": {},
   "source": [
    "# 🚶‍♂️ The Validation, by walking in the forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716547f-2cae-4727-a87d-93e3529b2a27",
   "metadata": {},
   "source": [
    "The \"*walk forward - validation*\" is the *technique* that will let us fit the model **which has generalized** the overall pattern in the data and is not *overfitted*.\n",
    "\n",
    "> But... we do have train-test-split right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f28d51-ac68-441e-b49f-e59b639c6101",
   "metadata": {},
   "source": [
    "## The problems\n",
    "\n",
    "## 1️⃣\n",
    "Till now, I have used the old school `ttsplit` technique to do the work... but that **again** *(\"again\" because that same problem rises in the common ML scenario)* creates problems because we are only partitionling the data in `2` parts and while testing, with the test data, we *tune the model* by ourselves — and hence the testing data becomes the \"in-sample\" data ie. the model might not have been generalized yet — only optimized for the training and testing data.\n",
    "\n",
    "## 2️⃣\n",
    "The solution was **to use the Cross-Validation**. The shiny, glittering, jargon, used by professors and paper readers term: \"Cross. Vali. Dation.\"\n",
    "\n",
    "Yeah, I mean literally this technique is like, when I read this... I've got it! But when it comes in different context... *I might wanna re-check it once*. Coming back to the problems: Can we use the Cross validation in the time series setting?\n",
    "\n",
    "> Of course... not!\n",
    "\n",
    "**Why?** <br>\n",
    "The reason is — the CV split *\"kind of randomized data\"*. While the time-series being a sequential data, ***we can't actually let it randomize it***. Consider this → 1, 2, 3, 4, 5 are the months in 2022. For 1st CV it will use 2, 3, 4, 5 as training and 1 for valucation. Fine, but then it will use 1, 3, 4, 5 and 2 for validation. There is a gap man!\n",
    "\n",
    "And also, the data points inside can be randomized as well!\n",
    "\n",
    "<img src=\"../images/no-nooo.gif\" hight=300 width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950217cd-bf8e-4178-b191-32bb74c21b9f",
   "metadata": {},
   "source": [
    "Okay got it... but what is the freakin' solution? How can I save my model not to **mug up** the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e876cb-b48f-4052-896c-aafa7a8fcab8",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d5b43-35eb-4d4b-9fe2-604d07e3c11a",
   "metadata": {},
   "source": [
    "## 📝 The WalkForward Validation\n",
    "*Notice, there is no \"CROSS\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d4cd7-30e0-4d2e-b6dd-de94bc6feb2a",
   "metadata": {},
   "source": [
    "<img src=\"../images/walk-forward-validation.png\" height=300 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30d515-1896-42b4-b1f3-b0ec8571994e",
   "metadata": {},
   "source": [
    "- We will start with *some minimum* amount of data.\n",
    "- Then we will forecast the `h` points.\n",
    "- That means: we will only take `h` sized validation set.\n",
    "- Then we will move some `offest` step ahead and repeat the same *(offset say, 1)*.\n",
    "- This will continue **till we reach to end** of the data.\n",
    "\n",
    "\n",
    "    `data`: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11\n",
    "    `h`: 3\n",
    "    \n",
    "    Step 1 — `train-set`: 1 | `validation-set`: 2, 3, 4\n",
    "    Step 1.1 — Train → forecast `h` and validate \n",
    "    Step 2 — `train-set`: 1, 2 | `validation-set`: 3, 4, 5\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    Step n - hth: — `train-set`: 1, 2, 3, 4, 5, 6, 7, 8 | `validation-set`: 9, 10, 11\n",
    "    (here 8th)\n",
    "\n",
    "😊 માજા આવી?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9b3f48-72d7-4ad8-843f-4d58135e4eb5",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a4426-9214-492b-8154-18d64ab637b9",
   "metadata": {},
   "source": [
    "### And ausaau <a href=\"https://dictionary.cambridge.org/media/english/uk_pron/u/uka/ukall/ukally_030.mp3\"> 🔊 </a>\n",
    "We can choose whether or not to take \"full data\" or not. Meaning, if in our case the data relies more on the recent past data, and we also want the *recent* data to be in the training... ***and instead of growing*** the data continuously by some `offset` term... we can keep the **datasize *constant***.\n",
    "\n",
    "*(you might want to click on the speaker)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5dbae-efc3-45cc-873b-4850eb1a7285",
   "metadata": {},
   "source": [
    "<img src=\"../images/fix-window.png\" height=300 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b35d8b5-46e4-4922-992b-f6744623c6bc",
   "metadata": {},
   "source": [
    "### It ausaau <a href=\"https://dictionary.cambridge.org/media/english/uk_pron/u/uka/ukall/ukally_030.mp3\"> 🔊 </a> possible to walk `h` steps\n",
    "Meaning, the \"bigger\" jumps!\n",
    "\n",
    "<img src=\"../images/h-steps.png\" height=300 width=500>\n",
    "\n",
    "- This *might* require you to manually calculate the `offset` size which **evenly** divides the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
