{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a099f98a-1ff2-4e31-9c29-48e660532289",
   "metadata": {},
   "source": [
    "# üî¢ Okay, but how about multiclass?\n",
    "*This will be a short book, which will introduce **another** activation function, just for the output layer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b05e1-e7d7-417e-a3b4-1f178d8247c0",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "We are gonna be using `softmax` function *(activation function)* on the output layer instead of the `sigmoid` because we are dealing with the multi-class classification. Softmax shows the *proportion* of all outputs so they add up to 1 and the highest or \"max\" is treated as a winning class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4012e90-ab24-4bc7-a612-05b3299069bd",
   "metadata": {},
   "source": [
    "## TL;WR\n",
    "Why a separate notebook for this simple stuff? Because I want to talk about a couple of things  expressively, not much in-depth but still a bit expressive, so I will need space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd24c6-b3bc-413f-b8bd-dd961ac1d37b",
   "metadata": {},
   "source": [
    "**Till now**:\n",
    "- We have seen that in the **hidden layers** we can use the sigmoid functions for the non-linear transformations.\n",
    "- As discussed in the previous notebooks, the sigmoid had its **problems** like the *vanishing gradient* and so forth, so...\n",
    "- We replaced with *(not exactly replaced... but used instead of)* ReLU in the hidden layers as another default activation function.\n",
    "- For output layer:\n",
    "    1. If there is regression. Don't use the activation function at all.\n",
    "    2. If there is classification *(binary)* use the **sigmoid**.\n",
    "    \n",
    "**So, now we are gonna talk about the \"Multiclass\" problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732b2a4-9730-47dc-83e8-10a72e88a56e",
   "metadata": {},
   "source": [
    "## üé¢ A situation\n",
    "\n",
    "<img src=\"../images/multiclass.png\" width=300 height=300>\n",
    "\n",
    "Here, we have the final layer calculation like...\n",
    "\n",
    "# $$a^{(L)} = W^{(L)T} z^{(L - 1)} + b^{(L)}$$\n",
    "\n",
    "Where, <br>\n",
    "As you know the $L$ represents the **last layer** of the NN.\n",
    "\n",
    "There, <br>\n",
    "$a^{(L)}:$ represents the vector of size $K$ where K is the **number of classes.**\n",
    "\n",
    "> Now, how to **map the outputs to the K different probabilities?**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b1880-ee62-47b2-a8bc-d5ed34681d40",
   "metadata": {},
   "source": [
    "# üç¶ Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b2cd7-14ef-4eb2-808f-2b4a7118bf6b",
   "metadata": {},
   "source": [
    "# $$\\text{softmax}(a_k) = \\frac{e^{(a_k)}}{\\displaystyle\\sum_{j=1}^K e^{(a_j)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a8c24-df64-4dd6-a5c7-ce6b1fd1613a",
   "metadata": {},
   "source": [
    "- In simple terms, it is represents the *proportion* of *that* output node out of 100.\n",
    "- Here we are performing $e$ or exponential, so we **confirm** that all output is **positive**.\n",
    "\n",
    "In NN:\n",
    "- \"Softmax\" is considered as an activation function, but **unlike** others like sigmoid, ReLU and tanh, it is **not meant** for the hidden layers.\n",
    "- Thus it is only for the output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e559b5-8bf4-4413-aa65-5eb2f6eefd34",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2e60a-de00-43db-bba7-3b8d05d3544b",
   "metadata": {},
   "source": [
    "## üìú Activation function summary\n",
    "\n",
    "| **Task**                  | **Activation function** |\n",
    "|---------------------------|-------------------------|\n",
    "| Regression                | None/Identity           |\n",
    "| Binary classification     | Sigmoid/Softmax         |\n",
    "| Multiclass classification | Softmax                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34a9bc-afbf-480b-894a-0e1feec7b06a",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae3c9a-38be-426d-9149-5bed74457913",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0259053-25bd-4703-a6cb-2f556fa95906",
   "metadata": {},
   "source": [
    "# That's done, and clear\n",
    "I know, I feel it, my hands are so eager to write the code down for the neural networks. In the very next book, we will start preparing the base of the NNs. That will be our starter. Then we will have a spark and finally our code will lit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
