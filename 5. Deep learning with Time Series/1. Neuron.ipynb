{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bc26d8-f012-4ad8-8b17-420bc2f5c495",
   "metadata": {},
   "source": [
    "# 🧠 Neuron\n",
    "*Sorry for re-using the used emoji again... but what should I do! There are no emojies related with a freakin neuron!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb5161-a8f9-4f0a-b349-b814e74d971f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0a6e1-f37e-41da-99bc-c220975df701",
   "metadata": {},
   "source": [
    "## 🧘‍♂️ \"A Logistic Regression as a Neuron\"\n",
    "\n",
    "Linear regression is a **fundamental**. Logistic regression **uses** linear regression internally. And a neuron can be **translated** into a logistic regression without a worry. \n",
    "\n",
    "*But how?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb7edc-e728-4fc1-bbeb-50affa8339de",
   "metadata": {},
   "source": [
    "> #### *Neurons are the fundamental unit of comuptation in neural netowrks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ebf47b-f372-4bb3-9b8f-b9c5399dcb3c",
   "metadata": {},
   "source": [
    "## 🔧 How is that built?\n",
    "So to understand the structure properly, we will first need to understand *what* it represents.\n",
    "\n",
    "> In one line: It is the ***transformed*** version of the linear equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205364cc-5522-4703-88c3-2399dc55e724",
   "metadata": {},
   "source": [
    "We have an equation:\n",
    "\n",
    "## $$\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$$\n",
    "\n",
    "Where, <br>\n",
    "$\\beta_0:$ Represents the value of $y$ when the $x_1$ and $x_2$ are 0. <br>\n",
    "$\\beta_{1, 2}:$ Are the coeffs to determine the **strength** of that input. The higher the absolute magnitude is, the more important that feature is. That can be both $\\pm$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694e089-f923-43c0-95a2-3e7c96b1b105",
   "metadata": {},
   "source": [
    "<img src=\"../images/nn.png\" height=300 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0b34c-d291-4a6c-88b0-9d48bd6d053e",
   "metadata": {},
   "source": [
    "- **Not** all neurons are connected with **equal** weights.\n",
    "- Some connections **are strong** while others **are weak**.\n",
    "\n",
    "In the neural network **terminoligy**:\n",
    "- The positive connections are ***\"excitatory (+)\"***.\n",
    "- The Negative connections are ***\"inhibitory (-)\"***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56bfe8-f2c3-4c5b-a5fc-82d2bca73e98",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f9e5b-2068-404c-a79d-a74d32d260eb",
   "metadata": {},
   "source": [
    "### 📶 The Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220856cc-9be6-433e-a708-36f196749251",
   "metadata": {},
   "source": [
    "The signal that is **passed** in the neurons have the ***special*** name: \"**Action Potential**\".\n",
    "\n",
    "It is a *spike* in the electrical potential. And we can call it an Activation function. It has some *threshold* value. Thus, *after summing up* the inputs, if the result is higher than the threshold, then the signal will be 1 otherwise 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482300d-7740-43ec-880b-e4d7c633b5dc",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d20d0-8d62-41a1-a58c-ad99f93079ad",
   "metadata": {},
   "source": [
    "### ➕ To Sum up\n",
    "As told in the earlier part of this book, a neuron behaves like a logistic regression. It indeed does. \n",
    "\n",
    "- There are inputs\n",
    "- They have weights\n",
    "- They form a linear form\n",
    "- They compute a single value\n",
    "- The single value is transformed through an actiona potential/activation function\n",
    "- The final result is stored in another neuron.\n",
    "\n",
    "This can be shown by the equation below:\n",
    "\n",
    "# $$p(y=1|x)$$\n",
    "### $$=$$\n",
    "# $$\\sigma(w^Tx + b)$$\n",
    "### $$=$$\n",
    "# $$\\sigma(\\displaystyle\\sum_{d=1}^D w_dx_d + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2794cd1-72fc-4a94-bc14-f0240045c509",
   "metadata": {},
   "source": [
    "Where, <br>\n",
    "$\\sigma:$ An activation function. <br>\n",
    "**Inner-side**:  is a linear equation.\n",
    "\n",
    "That's it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a590e0-711d-4aa0-a869-75f91eb78108",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5330b-7dc3-446b-8032-bd7bfcada644",
   "metadata": {},
   "source": [
    "# We are learning Deep Learning!\n",
    "In the next book, we will take a dive into the \"Forward-Propogation\". I know these topics are fimiliar but still for my future version I might want to save it in a referable format. So, there is will be. \n",
    "\n",
    "We will soon take some practical examples, don't worry. We are just sharpening our knife."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
