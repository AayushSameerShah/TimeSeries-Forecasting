{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490291f3-f05e-4554-89a1-1d90a348daee",
   "metadata": {},
   "source": [
    "# üìê Architecture \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44e1a4-d637-402e-b392-ef7d90bebf78",
   "metadata": {},
   "source": [
    "Convolution NN is crazy and amazing. Here we will break the NN into 2 parts or 2 stages. First is something **that we are interested in** and the second one is what we already know about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3084d-f5c8-4333-9775-51cf7c4d6d0a",
   "metadata": {},
   "source": [
    "<img src=\"../images/cnn-arc.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729322cc-f396-41a5-aa3a-2ca4bcb43687",
   "metadata": {},
   "source": [
    "**Part 1:** \n",
    "- Is a **feature transformer** as it usually is in the regular ANN.\n",
    "- This time, it is made ***specially*** for the images.\n",
    "- It is filled with **convolution** and **pooling** layers.\n",
    "- Here each convolution layer **extracts** some feature from the image and forwards that information to the next layers.\n",
    "\n",
    "**Part 2:** \n",
    "- Is made of a **bunch of regular dense** layers.\n",
    "- They perform **non-linear** transformations which they generally do.\n",
    "- These layers are called \"**fully connected layers**\" because they connect all convolutions and pooling output in the single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671301d-d9bc-42fd-b00a-c599d4a1a489",
   "metadata": {},
   "source": [
    "## üèä Pooling\n",
    "*Nah, the emoji isn't perfect.*\n",
    "\n",
    "> Pooling, on the high-level performs the ***downsampling*** operation. It **shrinks** the output of the previous conv later.\n",
    "\n",
    "<img src=\"../images/pooling.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49400266-995e-4815-8478-062b2c33fd4c",
   "metadata": {},
   "source": [
    "#### `2` kinds of pooling:\n",
    "1. Max *(more common)*\n",
    "2. Average\n",
    "\n",
    "<img src=\"../images/pooling-types.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae61a57-18f4-42f0-98d6-8ca7750e94d0",
   "metadata": {},
   "source": [
    "The given example is of **max** pooling. There is the similar operation done in the average case. What can that be, is an super easy guess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a628ec-a1ca-4b50-bf5d-fcceef5330a0",
   "metadata": {},
   "source": [
    "#### But why?\n",
    "- **Practical**: Less data to process, speedy\n",
    "- **Translational Invariance**: With pooling the feature can be highlighted. \n",
    "    - Meaning, in the simple image a nose can be anywhere right?\n",
    "    - Pooling says: *\"I don't care **where** the feature is, I care that **it does** exist.\"*\n",
    "    - Thus, it helps the model to be **more generalize**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d92871-6ad7-497b-855e-7312d1751f12",
   "metadata": {},
   "source": [
    "<img src=\"../images/transational0invariance.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d0274-a0a0-482e-9146-46c7323b0cb9",
   "metadata": {},
   "source": [
    "In the image above ‚Üë we have the letter \"A\". But while training, the NN will get the shape \"A\" in different places, so it will start looking for \"A\" like:\n",
    "\n",
    "> *hey, if the point is on 5th row and 55th col and 556th row and 6678th col, then it is A*.\n",
    "\n",
    "This simply isn't a good generalized model. If \"A\" happens to be somewhere else, then this will cause a problem. This is **where pooling helps**. It reduces the ***translational invariance***.\n",
    "___\n",
    "\n",
    "Here is a nice [question](https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo) thread that discusses the same topic and the image below shows the same problem:\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/iY5n5.png\" height=700 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9de617-996a-4809-835d-9aa4e2a8bf84",
   "metadata": {},
   "source": [
    "###  Unlike convolution...\n",
    "\n",
    "1. Pooling layer ***can be*** of uneven sizes. Meaning, it can be of $3\\times2$ or $5\\times2$ etc.\n",
    "    - Well convolution also ***can be***, but that's not much conventional.\n",
    "2. Pooling layer has possibility to **overlap**.\n",
    "    - Here we can set the hyperparameter \"stride\" to control the overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df9fde-9072-44f6-8aed-bcd13c521091",
   "metadata": {},
   "source": [
    "<img src=\"../images/pooling-types.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45557180-2eb4-4373-8e22-a8c79b37b95e",
   "metadata": {},
   "source": [
    "This image has:\n",
    "- **Pooling**: 2x2 | **Stride**:2\n",
    "    - That's why the boxes **didn't** overlap\n",
    "\n",
    "If:\n",
    "- **Pooling**: 2x2 | **Stride**:1\n",
    "    - Then the box **would** overlap\n",
    "- **Pooling**: 3x3 | **Stride**:2\n",
    "    - Then the box **would** overlap\n",
    "- **Pooling**: 3x3 | **Stride**:3\n",
    "     - Then the box **wouldn't** overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db385ac-83f4-42b6-845e-d04831e2d9c7",
   "metadata": {},
   "source": [
    "I think, this covers a pretty goo understanding of how the stride works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b4fcb-f73f-4570-98b7-6f43131d8155",
   "metadata": {},
   "source": [
    "## üå¥ CNN learns hierarchically\n",
    "<img src=\"../images/c-pool-c-pool.png\" height=400 width=500>\n",
    "\n",
    "This answers the question: *\"why conv layer - pooling - conv - pooling...?\"*\n",
    "\n",
    "The reason is: CNN learns hierarchically. Research has shown that the NN learns the **basic** features like basic strokes first, then looks for **higher order** features and so on. So, each feature *(here image)* has to be *pooled*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f033392-cdd9-4c50-ba4c-54647a9ccca0",
   "metadata": {},
   "source": [
    "#### After pooling..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d81688-20b9-497b-9a30-4fc0a92abdcc",
   "metadata": {},
   "source": [
    "<img src=\"../images/after-pooling.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4eca8-89af-4542-83f8-81a478401126",
   "metadata": {},
   "source": [
    "The image size shrinks. **But** the filter size stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc981b-ff6e-45bd-a7d3-5dd69e9a92d4",
   "metadata": {},
   "source": [
    "<img src=\"../images/increased-relation.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2afbd-aa5c-438b-96db-aa8aae60f4b4",
   "metadata": {},
   "source": [
    "See? How first in the ***bigger*** image the filter was able to learn the small stuff like strokes and then as it is progressing, it has started learning the bigger shapes like **whole face**. \n",
    "\n",
    "> ***This is what leads the CNN to learn the features hierarchically.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66dbfd1-f81a-4232-ad38-eca97791b9dc",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Losing information?\n",
    "On each iteration in the the deep layers, we are *shrinking* the image. Thus, there is *some* loss.\n",
    "- Since, we **care** about the feature's existence, we get some features which **indecates** that there is the feature but **don't know** where it is.\n",
    "- Since the first layer, we start finding the feature and if we get that, we perform other convolutions followed by pooling carrying that feature in the upcoming layers.\n",
    "- The ***spatial*** information decreases at each layer, **but** the number of **features** increases.\n",
    "- So, we don't care **where** the feature was found, but we care that it **was** found.\n",
    "- Hence, the number of feature maps increases. There can be many features at once.\n",
    "    > **Feature maps** are the output of one layer propogated to another layer. If you recall, we are appending all featuremaps into a single image making the image a 3-dimension. See the image below to get the idea... <br> <br>  <br><img src=\"../images/featuremap-increases.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120bfa67-1b41-4565-aee1-31a510df1f2f",
   "metadata": {},
   "source": [
    "## üëç Rule of Thumb\n",
    "We studied ANN, right? There weren't many hyperparameters. There were, indeed more than machine learning, but on our side, it wasn't a very high number. **Here**, we have too much.\n",
    "\n",
    "- Choose filter size\n",
    "- Number of feature maps\n",
    "- Number of layers\n",
    "- Pool size\n",
    "- Pool mode\n",
    "\n",
    "...\n",
    "\n",
    "### Guideline\n",
    "There is a general pattern that many people follow in this space. So, we can use that as our starting point.\n",
    "- Small filter size relative to the image: $3 \\times 3, 5 \\times 5, 7\\times 7$\n",
    "- Repeat: Conv ‚Üí Pool ‚Üí Conv ‚Üí Pool\n",
    "- Increase # of feature maps: 32, 64, 128, 128...\n",
    "- *Read lots of papers!*\n",
    "\n",
    "\n",
    "*(The given guidelines are from the lecture itself, I have not altered anything)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859766d-1409-4c41-95db-77c64f780f66",
   "metadata": {},
   "source": [
    "## üòÆ‚Äçüí®  Convolution can have \"stride\"\n",
    "What's the case. Let's understand.\n",
    "\n",
    "- **Striding** means, we ***skip*** the pixels and continue learning.\n",
    "- We do it because generally in an imaged, the **neighboring pixels are almost always highly correlated**.\n",
    "- So, learning the same information again and again will take more computation, so instead we just skip those pixels\n",
    "- **Doing so** will ***reduce*** the size of an image!\n",
    "\n",
    "Have a look at this animation:\n",
    "\n",
    "<img src=\"../images/convolution-strided.gif\" height=300 width=300>\n",
    "\n",
    "So, **with stride** we already are shrinking an image. The researchers have found out that \"using stride in the convolution and *not* using pooling layer, works more efficient or just as well\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbaaed-fe23-48a8-bbd4-71a3c1305c5c",
   "metadata": {},
   "source": [
    "<img src=\"../images/stride-vs-pool.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b49e3-76f3-41e5-81f1-45f8ee79bb8f",
   "metadata": {},
   "source": [
    "## üìîüìï What about the 2nd part?\n",
    "The dense layer?\n",
    "\n",
    "That fully connected layer will have to take the **image** as a **flattened** version (N x T x D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad382f7b-f6d9-4b62-b607-3dfaa88916d3",
   "metadata": {},
   "source": [
    "## ‚óºÔ∏è‚ñ™Ô∏è‚óªÔ∏è‚ñ´Ô∏è Different shaped images?\n",
    "For different sized shape... say:\n",
    "- An image of 32 x 32 is passed and after 4 convolutions it became size 4 x 4\n",
    "    - Now, with 10 feature maps we will have 4 x 4 x 10 = 160 shape vector (fully connected)\n",
    "- If another feature with 64 x 64 is passed and is after 4 convolutions it became size 8 x 8 *(since it was bigger)*\n",
    "    - Now, with 10 feature maps we will have 8 x 8 x 10 = 640 shape vector (fully connected)\n",
    "    \n",
    "We can see that **it results** in the different shape of the output. Which is not what NN can support! There can by any number of output then. So, we have something called **global max / average pooling**.\n",
    "\n",
    "The global pooling takes the max / average across all channels in the image and results in the predefined vector format. <br>\n",
    "<img src=\"../images/global-pooling.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec081aa-2667-4b61-a3b0-6252ecb98c9b",
   "metadata": {},
   "source": [
    "The example given above is of global pooling which takes `1` max out of all channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b8842-8aaf-4fbc-b23b-73165116c779",
   "metadata": {},
   "source": [
    "## üìë Summary\n",
    "\n",
    "<img src=\"../images/cnn-summary.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d13e4-4e9b-4d83-8a1e-2c3808a7e0fb",
   "metadata": {},
   "source": [
    "There, you can see:\n",
    "- We start with $28 \\times 28 \\times 1$ *(grey)* image.\n",
    "- That goes into the $5 \\times 5$ convolution with 32 feature maps *(meaning apply 32 convolutions)*.\n",
    "- That results the original $28 \\times 28$ image into $24 \\times 24 \\times 32$ *(where 32 is the number of feature maps)*.\n",
    "- The result $24 \\times 24 \\times 32$ is then applied to the pooling of $2 \\times 2$ which results in the **shrinked** image of $12 \\times 12 \\times 32$.\n",
    "- Again, we are applying the convolution of $5 \\times 5$ but here with 64 feature maps *(meaning applying 64 convolutions)*.\n",
    "    > Thing to note here, <br> <br> Since in the previous layer we had $12 \\times 12 \\times 32$ image, in the next layer *(after 5x5 convolution)* it should be $8 \\times 8 \\times 32 \\times 64$, right? But nah! Each convolution (out of 64, again each.) will sum the channels into a single. Thus, $8 \\times 8 \\times 32$ for each convolution will become $8 \\times 8$ and thus for 64 fmaps, we will have $8 \\times 8 \\times 32$.\n",
    "- Finally we apply the flatten layer and the things are straight forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891f2a4-9abd-4dc9-9611-c8e82cacd2a7",
   "metadata": {},
   "source": [
    "<img src=\"../images/stepwise-cnn.png\" height=500 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537788f3-54ad-419b-864a-c939baea453c",
   "metadata": {},
   "source": [
    "Do read the steps above and see the illustration to understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba3783-3087-40c8-ba1b-6e4f4b4b809e",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2482b7-45c0-43b1-adbf-8b4ea2dcbe5b",
   "metadata": {},
   "source": [
    "# Amazing \n",
    "Let's catch up in the next book where we will write our first CNN code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
